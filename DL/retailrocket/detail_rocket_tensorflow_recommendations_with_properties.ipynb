{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:53:41.184519Z",
     "start_time": "2023-12-01T07:53:38.291809Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install -q tensorflow-recommenders\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Embedding, Dense, StringLookup\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.src.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:53:41.194633Z",
     "start_time": "2023-12-01T07:53:41.185777Z"
    }
   },
   "id": "89849bbf9d8a3d88"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         visitorid  itemid  event_mapped\n2482751    1046539  373805             1\n2204101    1383579  287405             1\n1964828     474264  153625             1\n2369299    1079433  287356             1\n640471      865010  113440             1\n...            ...     ...           ...\n560807      269455  459735             1\n1158627     228194  160499             1\n2074769     435897  346429             1\n1911438    1099927  261011             1\n1975046     747991  415715             1\n\n[10000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>visitorid</th>\n      <th>itemid</th>\n      <th>event_mapped</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2482751</th>\n      <td>1046539</td>\n      <td>373805</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2204101</th>\n      <td>1383579</td>\n      <td>287405</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1964828</th>\n      <td>474264</td>\n      <td>153625</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2369299</th>\n      <td>1079433</td>\n      <td>287356</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>640471</th>\n      <td>865010</td>\n      <td>113440</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>560807</th>\n      <td>269455</td>\n      <td>459735</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1158627</th>\n      <td>228194</td>\n      <td>160499</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2074769</th>\n      <td>435897</td>\n      <td>346429</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1911438</th>\n      <td>1099927</td>\n      <td>261011</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1975046</th>\n      <td>747991</td>\n      <td>415715</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "        itemid categoryid available  \\\n6            6       1091         1   \n31          32       1173         0   \n40          42         84         1   \n137        147        646         1   \n153        163        407         0   \n...        ...        ...       ...   \n416890  466685       1400         1   \n416941  466740        967         0   \n416960  466760       1549         0   \n417047  466861       1051         0   \n417050  466864        373         1   \n\n                                               properties  \\\n6       [112, 159, 19, 202, 227, 28, 283, 364, 521, 55...   \n31      [1036, 1052, 1066, 112, 159, 202, 227, 230, 28...   \n40      [1036, 1052, 1066, 112, 159, 202, 227, 230, 28...   \n137     [1092, 112, 159, 202, 283, 348, 364, 461, 491,...   \n153     [112, 159, 202, 227, 283, 364, 376, 397, 483, ...   \n...                                                   ...   \n416890  [102, 1028, 112, 159, 202, 227, 275, 28, 283, ...   \n416941  [1008, 1036, 112, 120, 140, 159, 202, 227, 283...   \n416960  [1036, 1066, 112, 159, 202, 210, 227, 230, 283...   \n417047  [1036, 1066, 112, 159, 202, 227, 230, 283, 300...   \n417050  [1036, 112, 152, 159, 202, 227, 230, 283, 348,...   \n\n                                          property_values  \n6       [679677, 519769, 1297729 n72.000 309206, 60935...  \n31      [726612, 1116693, n973.200 424566, 679677, 519...  \n40      [726612, 1116693, n68.400 424566, 679677, 5197...  \n137     [291010, 679677, 519769, 229273 388993 1246541...  \n153     [679677, 519769, 62992 n7440.000 925243, 92933...  \n...                                                   ...  \n416890  [769062, 769062, 679677, 519769, 1109436 45934...  \n416941  [124229 n336.000 1144008, 1154859, 679677, 115...  \n416960  [1318567, n720.000 424566, 679677, 519769, 123...  \n417047  [1318567, 732011 424566, 679677, 519769, 10769...  \n417050  [1154859, 679677, 1071492, 519769, 1262739 205...  \n\n[7705 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>itemid</th>\n      <th>categoryid</th>\n      <th>available</th>\n      <th>properties</th>\n      <th>property_values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>1091</td>\n      <td>1</td>\n      <td>[112, 159, 19, 202, 227, 28, 283, 364, 521, 55...</td>\n      <td>[679677, 519769, 1297729 n72.000 309206, 60935...</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>32</td>\n      <td>1173</td>\n      <td>0</td>\n      <td>[1036, 1052, 1066, 112, 159, 202, 227, 230, 28...</td>\n      <td>[726612, 1116693, n973.200 424566, 679677, 519...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>42</td>\n      <td>84</td>\n      <td>1</td>\n      <td>[1036, 1052, 1066, 112, 159, 202, 227, 230, 28...</td>\n      <td>[726612, 1116693, n68.400 424566, 679677, 5197...</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>147</td>\n      <td>646</td>\n      <td>1</td>\n      <td>[1092, 112, 159, 202, 283, 348, 364, 461, 491,...</td>\n      <td>[291010, 679677, 519769, 229273 388993 1246541...</td>\n    </tr>\n    <tr>\n      <th>153</th>\n      <td>163</td>\n      <td>407</td>\n      <td>0</td>\n      <td>[112, 159, 202, 227, 283, 364, 376, 397, 483, ...</td>\n      <td>[679677, 519769, 62992 n7440.000 925243, 92933...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>416890</th>\n      <td>466685</td>\n      <td>1400</td>\n      <td>1</td>\n      <td>[102, 1028, 112, 159, 202, 227, 275, 28, 283, ...</td>\n      <td>[769062, 769062, 679677, 519769, 1109436 45934...</td>\n    </tr>\n    <tr>\n      <th>416941</th>\n      <td>466740</td>\n      <td>967</td>\n      <td>0</td>\n      <td>[1008, 1036, 112, 120, 140, 159, 202, 227, 283...</td>\n      <td>[124229 n336.000 1144008, 1154859, 679677, 115...</td>\n    </tr>\n    <tr>\n      <th>416960</th>\n      <td>466760</td>\n      <td>1549</td>\n      <td>0</td>\n      <td>[1036, 1066, 112, 159, 202, 210, 227, 230, 283...</td>\n      <td>[1318567, n720.000 424566, 679677, 519769, 123...</td>\n    </tr>\n    <tr>\n      <th>417047</th>\n      <td>466861</td>\n      <td>1051</td>\n      <td>0</td>\n      <td>[1036, 1066, 112, 159, 202, 227, 230, 283, 300...</td>\n      <td>[1318567, 732011 424566, 679677, 519769, 10769...</td>\n    </tr>\n    <tr>\n      <th>417050</th>\n      <td>466864</td>\n      <td>373</td>\n      <td>1</td>\n      <td>[1036, 112, 152, 159, 202, 227, 230, 283, 348,...</td>\n      <td>[1154859, 679677, 1071492, 519769, 1262739 205...</td>\n    </tr>\n  </tbody>\n</table>\n<p>7705 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "def map_event(event):\n",
    "    if event == 'view':\n",
    "        return 1\n",
    "    elif event == 'addtocart':\n",
    "        return 2\n",
    "    elif event == 'transaction':\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "basepath = '../../datasets/preprocessed_datasets/retailrocket/'\n",
    "events = pd.read_pickle(basepath +'events_10k.pkl')\n",
    "events['event_mapped'] = events['event'].apply(map_event)\n",
    "events = events.drop(columns=['timestamp', 'event', 'transactionid', 'datetime'])\n",
    "item_properties = pd.read_pickle(basepath +'item_data_extracted.pkl')\n",
    "\n",
    "item_properties = item_properties[item_properties['itemid'].isin(events.itemid.unique())]\n",
    "\n",
    "display(events, item_properties)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:08.255292Z",
     "start_time": "2023-12-01T07:53:41.189605Z"
    }
   },
   "id": "e008bf18a8e80e4f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "      visitorid  itemid  event_mapped  \\\n0       1046539  373805             1   \n1       1383579  287405             1   \n2        474264  153625             1   \n3       1079433  287356             1   \n4        865010  113440             1   \n...         ...     ...           ...   \n9995     269455  459735             1   \n9996     228194  160499             1   \n9997     435897  346429             1   \n9998    1099927  261011             1   \n9999     747991  415715             1   \n\n                                             properties  \\\n0     [1075, 112, 159, 202, 25, 283, 364, 6, 678, 69...   \n1     [112, 159, 202, 227, 28, 283, 293, 30, 348, 36...   \n2     [1092, 112, 159, 202, 283, 348, 349, 364, 461,...   \n3     [1066, 112, 159, 202, 227, 230, 283, 307, 364,...   \n4     [1000, 1036, 112, 159, 202, 227, 230, 283, 324...   \n...                                                 ...   \n9995  [1000, 1036, 112, 159, 202, 227, 230, 283, 364...   \n9996  [112, 159, 202, 227, 28, 283, 293, 30, 348, 36...   \n9997  [112, 159, 202, 283, 348, 364, 461, 506, 591, ...   \n9998  [1036, 112, 152, 159, 202, 227, 230, 283, 348,...   \n9999                                                NaN   \n\n                                        property_values  \n0     [[n120.000 1029109 n1200.000 1029109, n120.000...  \n1     [679677, 519769, 1288624 n4944.000 969301, 282...  \n2     [291010, 679677, 519769, 695463, 726714 422480...  \n3     [732011 424566, 679677, 519769, 1213269, 12143...  \n4     [871215 1022520, 726612, 679677, 519769, 75240...  \n...                                                 ...  \n9995  [237874 1022520, 726612, 679677, 519769, 12751...  \n9996  [679677, 519769, 592578, 188678 607315, 150169...  \n9997  [679677, 519769, 898578, 726714 150169 210500 ...  \n9998  [1154859, 679677, 769062, 519769, 875827 76692...  \n9999                                                NaN  \n\n[10000 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>visitorid</th>\n      <th>itemid</th>\n      <th>event_mapped</th>\n      <th>properties</th>\n      <th>property_values</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1046539</td>\n      <td>373805</td>\n      <td>1</td>\n      <td>[1075, 112, 159, 202, 25, 283, 364, 6, 678, 69...</td>\n      <td>[[n120.000 1029109 n1200.000 1029109, n120.000...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1383579</td>\n      <td>287405</td>\n      <td>1</td>\n      <td>[112, 159, 202, 227, 28, 283, 293, 30, 348, 36...</td>\n      <td>[679677, 519769, 1288624 n4944.000 969301, 282...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>474264</td>\n      <td>153625</td>\n      <td>1</td>\n      <td>[1092, 112, 159, 202, 283, 348, 349, 364, 461,...</td>\n      <td>[291010, 679677, 519769, 695463, 726714 422480...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1079433</td>\n      <td>287356</td>\n      <td>1</td>\n      <td>[1066, 112, 159, 202, 227, 230, 283, 307, 364,...</td>\n      <td>[732011 424566, 679677, 519769, 1213269, 12143...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>865010</td>\n      <td>113440</td>\n      <td>1</td>\n      <td>[1000, 1036, 112, 159, 202, 227, 230, 283, 324...</td>\n      <td>[871215 1022520, 726612, 679677, 519769, 75240...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>269455</td>\n      <td>459735</td>\n      <td>1</td>\n      <td>[1000, 1036, 112, 159, 202, 227, 230, 283, 364...</td>\n      <td>[237874 1022520, 726612, 679677, 519769, 12751...</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>228194</td>\n      <td>160499</td>\n      <td>1</td>\n      <td>[112, 159, 202, 227, 28, 283, 293, 30, 348, 36...</td>\n      <td>[679677, 519769, 592578, 188678 607315, 150169...</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>435897</td>\n      <td>346429</td>\n      <td>1</td>\n      <td>[112, 159, 202, 283, 348, 364, 461, 506, 591, ...</td>\n      <td>[679677, 519769, 898578, 726714 150169 210500 ...</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>1099927</td>\n      <td>261011</td>\n      <td>1</td>\n      <td>[1036, 112, 152, 159, 202, 227, 230, 283, 348,...</td>\n      <td>[1154859, 679677, 769062, 519769, 875827 76692...</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>747991</td>\n      <td>415715</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "events = pd.merge(events, item_properties[['itemid', 'properties', 'property_values']],\n",
    "                                   left_on='itemid',\n",
    "                                   right_on='itemid',\n",
    "                                   how='left')\n",
    "display(events)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:08.269613Z",
     "start_time": "2023-12-01T07:54:08.256887Z"
    }
   },
   "id": "312fe10f0cd81ce9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "\n",
    "# Assuming events is your DataFrame\n",
    "\n",
    "def process_property_values(series):\n",
    "    def process_item(item):\n",
    "        if isinstance(item, float):  # Check if the item is a float\n",
    "            return item  # Return the float as is\n",
    "        elif isinstance(item, list):  # Check if the item is a list\n",
    "            return [sub_item[0] if isinstance(sub_item, list) else sub_item for sub_item in item]\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "    return series.apply(lambda x: process_item(x))\n",
    "\n",
    "# Process 'properties' and 'property_values'\n",
    "events['properties'] = events['properties'].apply(lambda x: str(x))\n",
    "events['property_values'] = process_property_values(events['property_values'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:08.425757Z",
     "start_time": "2023-12-01T07:54:08.318335Z"
    }
   },
   "id": "309086c9d868b28f"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "events['properties'] = events['properties'].apply(lambda x: str(x))\n",
    "events['property_values'] = events['property_values'].apply(lambda x: str(x))\n",
    "\n",
    "def get_padded_sequence(values):\n",
    "    \n",
    "    string_lookup = StringLookup()\n",
    "    string_lookup.adapt(values)\n",
    "\n",
    "    tokenizer_properties = Tokenizer(num_words=string_lookup.vocabulary_size(), oov_token=\"<OOV>\")\n",
    "    tokenizer_properties.fit_on_texts(values)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    sequences = tokenizer_properties.texts_to_sequences(values)\n",
    "\n",
    "    # Pad sequences\n",
    "    return pad_sequences(sequences, padding='post')\n",
    "\n",
    "padded_sequence_properties = get_padded_sequence(events['properties'])\n",
    "padded_sequence_property_values = get_padded_sequence(events['property_values'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:12.507108Z",
     "start_time": "2023-12-01T07:54:08.329492Z"
    }
   },
   "id": "33c2292c813bc54c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0       [property\\n1075    n120.000 1029109 n1200.000 ...\n1       ['679677', '519769', '1288624 n4944.000 969301...\n2       ['291010', '679677', '519769', '695463', '7267...\n3       ['732011 424566', '679677', '519769', '1213269...\n4       ['871215 1022520', '726612', '679677', '519769...\n                              ...                        \n9995    ['237874 1022520', '726612', '679677', '519769...\n9996    ['679677', '519769', '592578', '188678 607315'...\n9997    ['679677', '519769', '898578', '726714 150169 ...\n9998    ['1154859', '679677', '769062', '519769', '875...\n9999                                                  nan\nName: property_values, Length: 10000, dtype: object"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events['property_values']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:12.507587Z",
     "start_time": "2023-12-01T07:54:12.501294Z"
    }
   },
   "id": "61ad2efc4331f266"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  7, 347, 210, ...,   0,   0,   0],\n       [ 19,  20,   1, ...,   0,   0,   0],\n       [377,  19,  20, ...,   0,   0,   0],\n       ...,\n       [ 19,  20,   1, ...,   0,   0,   0],\n       [106,  19,   5, ...,   0,   0,   0],\n       [171,   0,   0, ...,   0,   0,   0]], dtype=int32)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sequence_property_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:12.507667Z",
     "start_time": "2023-12-01T07:54:12.504119Z"
    }
   },
   "id": "91b0fa0528aba364"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Create a tf.data.Dataset from the interaction data\n",
    "#todo check how to use array of ids\n",
    "interaction_dataset = tf.data.Dataset.from_tensor_slices({\n",
    "    'visitorid': events['visitorid'].values,\n",
    "    'itemid': events['itemid'].values,\n",
    "    'event': events['event_mapped'].values,\n",
    "    'properties': padded_sequence_properties,  # Convert to list\n",
    "    'property_values': padded_sequence_property_values,\n",
    "})\n",
    "#variables \n",
    "dataset_len = events.shape[0]\n",
    "test_len = math.ceil(dataset_len * 0.2)\n",
    "train_len = dataset_len - test_len\n",
    "\n",
    "metrics_batchsize = 16\n",
    "train_batch_size = 128\n",
    "test_batch_size = 64\n",
    "random_seed = 27"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:12.529640Z",
     "start_time": "2023-12-01T07:54:12.509729Z"
    }
   },
   "id": "c089ac7f029dc324"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<_TensorSliceDataset element_spec={'visitorid': TensorSpec(shape=(), dtype=tf.int64, name=None), 'itemid': TensorSpec(shape=(), dtype=tf.int64, name=None), 'event': TensorSpec(shape=(), dtype=tf.int64, name=None), 'properties': TensorSpec(shape=(54,), dtype=tf.int32, name=None), 'property_values': TensorSpec(shape=(767,), dtype=tf.int32, name=None)}>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interaction_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:12.529955Z",
     "start_time": "2023-12-01T07:54:12.519604Z"
    }
   },
   "id": "1271391f90ebc74d"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<_TakeDataset element_spec={'visitorid': TensorSpec(shape=(), dtype=tf.int64, name=None), 'itemid': TensorSpec(shape=(), dtype=tf.int64, name=None), 'event': TensorSpec(shape=(), dtype=tf.int64, name=None), 'properties': TensorSpec(shape=(54,), dtype=tf.int32, name=None), 'property_values': TensorSpec(shape=(767,), dtype=tf.int32, name=None)}>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<_TakeDataset element_spec={'visitorid': TensorSpec(shape=(), dtype=tf.int64, name=None), 'itemid': TensorSpec(shape=(), dtype=tf.int64, name=None), 'event': TensorSpec(shape=(), dtype=tf.int64, name=None), 'properties': TensorSpec(shape=(54,), dtype=tf.int32, name=None), 'property_values': TensorSpec(shape=(767,), dtype=tf.int32, name=None)}>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train test split\n",
    "tf.random.set_seed(random_seed)\n",
    "# shuffled = interaction_dataset.shuffle(len, seed=random_seed, reshuffle_each_iteration=False)\n",
    "# train = shuffled.take(train_len)\n",
    "# test = shuffled.skip(train_len).take(test_len)\n",
    "shuffled = interaction_dataset.shuffle(dataset_len, seed=random_seed, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(train_len)\n",
    "test = shuffled.skip(train_len).take(test_len)\n",
    "display(train, test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:12.620536Z",
     "start_time": "2023-12-01T07:54:12.525726Z"
    }
   },
   "id": "825eec174445be8c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Unique customer and product identifiers\n",
    "unique_visitor_ids = np.array(events[\"visitorid\"].unique())\n",
    "unique_item_ids = np.array(events[\"itemid\"].unique())\n",
    "\n",
    "\n",
    "visitor_ids_vocabulary = tf.keras.layers.IntegerLookup(mask_token=None)\n",
    "visitor_ids_vocabulary.adapt(unique_visitor_ids)\n",
    "\n",
    "item_ids_vocabulary = tf.keras.layers.IntegerLookup(mask_token=None)\n",
    "item_ids_vocabulary.adapt(unique_item_ids)\n",
    "\n",
    "\n",
    "properties_vocabulary = tf.keras.layers.IntegerLookup(mask_token=None)\n",
    "properties_vocabulary.adapt(padded_sequence_properties)\n",
    "\n",
    "property_values_vocabulary = tf.keras.layers.IntegerLookup(mask_token=None)\n",
    "property_values_vocabulary.adapt(padded_sequence_property_values)\n",
    "\n",
    "\n",
    "# print(unique_product_ids.shape[0], unique_customer_ids.shape[0], unique_product_ids, unique_product_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:14.428046Z",
     "start_time": "2023-12-01T07:54:12.621519Z"
    }
   },
   "id": "2e2625e63cf53fce"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:14.428221Z",
     "start_time": "2023-12-01T07:54:14.420081Z"
    }
   },
   "id": "580fc3478a356294"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Define a model using TensorFlow Recommenders\n",
    "product_ids_dataset = tf.data.Dataset.from_tensor_slices(unique_item_ids)\n",
    "class RetailModel(tfrs.Model):\n",
    "\n",
    "    def __init__(self, unique_item_ids_vocab, unique_visitor_ids_vocab, unique_properties_vocab, unique_property_values_vocab, embedding_dimension=32):\n",
    "        super().__init__()\n",
    "        # Set up user and product representations\n",
    "        self.user_embedding = tf.keras.Sequential([\n",
    "            unique_visitor_ids_vocab,\n",
    "            Embedding(unique_visitor_ids_vocab.vocabulary_size(), embedding_dimension)\n",
    "        ])\n",
    "        self.product_embedding = tf.keras.Sequential([\n",
    "            unique_item_ids_vocab,\n",
    "            Embedding(unique_item_ids_vocab.vocabulary_size(), embedding_dimension)\n",
    "        ])\n",
    "\n",
    "        max_features = 10000  # Adjust as needed\n",
    "        max_len = 20          # Adjust as needed\n",
    "        self.text_vectorization = tf.keras.layers.TextVectorization(\n",
    "            max_tokens=max_features,\n",
    "            output_mode='int',\n",
    "            output_sequence_length=max_len\n",
    "        )\n",
    "\n",
    "\n",
    "        # Textual model\n",
    "        self.textual_properties_model = tf.keras.Sequential([\n",
    "            self.text_vectorization,\n",
    "            Embedding(input_dim=unique_property_values_vocab.vocabulary_size(), output_dim=embedding_dimension),\n",
    "            tf.keras.layers.Reshape((1, embedding_dimension)),  # Adding the 'timesteps' dimension\n",
    "            LSTM(64),\n",
    "            Dense(64, activation='relu')\n",
    "        ])\n",
    "        \n",
    "        # Textual model\n",
    "        self.textual_property_values_model = tf.keras.Sequential([\n",
    "            self.text_vectorization,\n",
    "            Embedding(input_dim=unique_property_values_vocab.vocabulary_size(), output_dim=embedding_dimension),\n",
    "            tf.keras.layers.Reshape((1, embedding_dimension)),\n",
    "            LSTM(64),\n",
    "            Dense(64, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.reduced_product_embeddings = Dense(64, activation='relu')\n",
    "        self.reduced_properties_embeddings = Dense(32, activation='relu')\n",
    "        self.reduced_property_values_embeddings = Dense(32, activation='relu')\n",
    "        \n",
    "        \n",
    "        # Set up a dense layer for the task.\n",
    "        self.dense_layer = Dense(128, input_shape=(256,), activation=\"relu\")\n",
    "\n",
    "        # Set up retrieval task and metrics\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(candidates=product_ids_dataset.batch(metrics_batchsize).map(self.product_model))\n",
    "        )\n",
    "        self.auc_metric = AUC(name='auc')\n",
    "        self.rmse_metric = tf.keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "        self.precision = tf.keras.metrics.Precision(name='precision')\n",
    "        self.recall = tf.keras.metrics.Recall(name='recall')\n",
    "\n",
    "    def product_model(self, product_ids):\n",
    "        return self.product_embedding(product_ids)\n",
    "\n",
    "    def dot_product_score(self, user, product):\n",
    "        \"\"\"\n",
    "        Computes the dot product between user and product embeddings to get the interaction score.\n",
    "        \"\"\"\n",
    "        return tf.reduce_sum(user * product, axis=1)\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # print(features)\n",
    "        print(list(features.keys()))\n",
    "        user_embeddings = self.user_embedding(features[\"visitorid\"])\n",
    "        product_embeddings = self.product_embedding(features[\"itemid\"])\n",
    "        properties_embeddings = self.textual_properties_model(features[\"properties\"])\n",
    "        property_values_embeddings = self.textual_property_values_model(features[\"property_values\"])\n",
    "\n",
    "        reduced_product_embeddings = self.reduced_product_embeddings(product_embeddings)\n",
    "        reduced_properties_embeddings = self.reduced_properties_embeddings(properties_embeddings)\n",
    "        reduced_property_values_embeddings = self.reduced_property_values_embeddings(property_values_embeddings)\n",
    "        \n",
    "        \n",
    "        # \n",
    "        # # Now concatenate\n",
    "        product_combined = tf.concat([reduced_product_embeddings, reduced_properties_embeddings, reduced_property_values_embeddings], axis=1)  # shape will be [None, 128]\n",
    "        \n",
    "\n",
    "\n",
    "        user_output = self.dense_layer(user_embeddings)\n",
    "        product_output = self.dense_layer(product_combined)\n",
    "        # This is a hypothetical function that returns the logits or scores\n",
    "        # This needs to be adapted based on your actual model\n",
    "        positive_logits = self.dot_product_score(user_output, product_output)\n",
    "\n",
    "        # Update RMSE\n",
    "        self.rmse_metric.update_state(y_true=features[\"event\"], y_pred=positive_logits)\n",
    "\n",
    "\n",
    "        return self.task(user_output, product_output)\n",
    "\n",
    "    def evaluate(self, validation_dataset, *args, **kwargs):\n",
    "        # Call the base class's evaluate method\n",
    "        \n",
    "    \n",
    "        # Compute additional metrics\n",
    "        for features in validation_dataset:\n",
    "            user_embeddings = self.user_embedding(features[\"visitorid\"])\n",
    "            product_embeddings = self.product_embedding(features[\"itemid\"])\n",
    "            \n",
    "            user_output = self.dense_layer(user_embeddings)\n",
    "            product_output = self.dense_layer(product_embeddings)\n",
    "            \n",
    "            positive_logits = self.dot_product_score(user_output, product_output)\n",
    "            # Assuming you have a binary \"label\" in your dataset indicating 1 for positive interaction and 0 for negative\n",
    "            self.precision.update_state(y_true=features[\"event\"], y_pred=positive_logits)\n",
    "            self.recall.update_state(y_true=features[\"event\"], y_pred=positive_logits)\n",
    "    \n",
    "        precision_result = self.precision.result().numpy()\n",
    "        recall_result = self.recall.result().numpy()\n",
    "    \n",
    "        # Calculate F1 score\n",
    "        if (precision_result + recall_result) != 0:\n",
    "            f1_score = 2 * (precision_result * recall_result) / (precision_result + recall_result)\n",
    "        else:\n",
    "            f1_score = 0.0\n",
    "    \n",
    "        # Reset the metrics for the next evaluation\n",
    "        # self.precision.reset_states()\n",
    "        # self.recall.reset_states()\n",
    "        base_results = super(RetailModel, self).evaluate(validation_dataset, *args, **kwargs)\n",
    "    \n",
    "        return base_results, [precision_result, recall_result, f1_score]  # or append additional results as needed\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:14.448628Z",
     "start_time": "2023-12-01T07:54:14.421794Z"
    }
   },
   "id": "378cf58a3061ecfc"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model = RetailModel(item_ids_vocabulary, visitor_ids_vocabulary, properties_vocabulary, property_values_vocabulary, embedding_dimension=128)\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001), metrics=[AUC()]) # using legacy instead of tf.keras.optimizers.Adagrad, because newer version is slow on m1/m2 macs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:14.837920Z",
     "start_time": "2023-12-01T07:54:14.437590Z"
    }
   },
   "id": "5ae0c3e4d7093bf1"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "['visitorid', 'itemid', 'event', 'properties', 'property_values']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/tensorflow_recommenders/models/base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"/var/folders/jp/3q1jwf6d5n11rx71jwtqcgv80000gn/T/ipykernel_69802/206799084.py\", line 75, in compute_loss\n        properties_embeddings = self.textual_properties_model(features[\"properties\"])\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/layers/preprocessing/text_vectorization.py\", line 588, in _preprocess\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'text_vectorization' (type TextVectorization).\n    \n    When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 54) with rank=2\n    \n    Call arguments received by layer 'text_vectorization' (type TextVectorization):\n      • inputs=tf.Tensor(shape=(None, 54), dtype=int32)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m cached_test \u001B[38;5;241m=\u001B[39m test\u001B[38;5;241m.\u001B[39mbatch(test_batch_size)\u001B[38;5;241m.\u001B[39mcache()\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# model.fit(cached_train, validation_data=cached_test, validation_freq=5, epochs=3) \u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcached_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/var/folders/jp/3q1jwf6d5n11rx71jwtqcgv80000gn/T/__autograph_generated_filef7kaef4r.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/tensorflow_recommenders/models/base.py:68\u001B[0m, in \u001B[0;36mModel.train_step\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Custom train step using the `compute_loss` method.\"\"\"\u001B[39;00m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mGradientTape() \u001B[38;5;28;01mas\u001B[39;00m tape:\n\u001B[0;32m---> 68\u001B[0m   loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(inputs, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     70\u001B[0m   \u001B[38;5;66;03m# Handle regularization losses as well.\u001B[39;00m\n\u001B[1;32m     71\u001B[0m   regularization_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlosses)\n",
      "Cell \u001B[0;32mIn[14], line 75\u001B[0m, in \u001B[0;36mRetailModel.compute_loss\u001B[0;34m(self, features, training)\u001B[0m\n\u001B[1;32m     73\u001B[0m user_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muser_embedding(features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvisitorid\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     74\u001B[0m product_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproduct_embedding(features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mitemid\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 75\u001B[0m properties_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtextual_properties_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mproperties\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m property_values_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtextual_property_values_model(features[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproperty_values\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     78\u001B[0m reduced_product_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreduced_product_embeddings(product_embeddings)\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/tensorflow_recommenders/models/base.py\", line 68, in train_step\n        loss = self.compute_loss(inputs, training=True)\n    File \"/var/folders/jp/3q1jwf6d5n11rx71jwtqcgv80000gn/T/ipykernel_69802/206799084.py\", line 75, in compute_loss\n        properties_embeddings = self.textual_properties_model(features[\"properties\"])\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/thomashuber/miniconda3/envs/m1-master-thesis/lib/python3.9/site-packages/keras/src/layers/preprocessing/text_vectorization.py\", line 588, in _preprocess\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'text_vectorization' (type TextVectorization).\n    \n    When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, 54) with rank=2\n    \n    Call arguments received by layer 'text_vectorization' (type TextVectorization):\n      • inputs=tf.Tensor(shape=(None, 54), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle, batch, and cache the data.\n",
    "cached_train = train.shuffle(dataset_len).batch(train_batch_size).cache()\n",
    "cached_test = test.batch(test_batch_size).cache()\n",
    "# Train the model\n",
    "# model.fit(cached_train, validation_data=cached_test, validation_freq=5, epochs=3) \n",
    "model.fit(cached_train, validation_freq=5, epochs=3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:14.926815Z",
     "start_time": "2023-12-01T07:54:14.839167Z"
    }
   },
   "id": "4add146de8d83cb0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "k = 5\n",
    "result_evaluate_train = model.evaluate(cached_train)\n",
    "result_evaluate_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T07:54:14.928547Z",
     "start_time": "2023-12-01T07:54:14.927222Z"
    }
   },
   "id": "f38fbc7489d6c8ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_evaluat_test = model.evaluate(cached_test)\n",
    "result_evaluat_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T07:54:14.928330Z"
    }
   },
   "id": "a90bfb5478e24540"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Name\\t\\tValue\")\n",
    "print(\"-\" * 30)\n",
    "for metric in model.metrics:\n",
    "    print(f\"{metric.name}\\t\\t{metric.result().numpy()}\")\n",
    "print(f\"AUC: {model.auc_metric.result().numpy()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T07:54:14.929970Z"
    }
   },
   "id": "2dc815d6547dfefe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample = next(iter(cached_test.take(1)))\n",
    "\n",
    "# Extract customer_id from the sample\n",
    "visitor_id = sample['visitorid'].numpy()\n",
    "visitor_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T07:54:14.931341Z"
    }
   },
   "id": "d293fa87e3fe5b52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_random_customer_from_test_data():\n",
    "    return next(iter(test.take(1)))['visitorid'].numpy()\n",
    "\n",
    "def display_item_ids(item_ids):\n",
    "    display(item_properties[item_properties.itemid.isin(item_ids) ])\n",
    "\n",
    "def display_products_by_visitor_id(visitor_id):\n",
    "    display_item_ids(events[events.visitorid == visitor_id].itemid.tolist())\n",
    "    \n",
    "\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.user_embedding)\n",
    "index.index_from_dataset(\n",
    "    product_ids_dataset.batch(100).map(lambda id: (id, model.product_model(id))))\n",
    "\n",
    "def predict_user(visitor_id):\n",
    "    print('predicting user: ', visitor_id)\n",
    "    print('user already bought following products: ')\n",
    "    display_products_by_visitor_id(visitor_id)\n",
    "    \n",
    "    score, predicted_product_ids = index(np.array([visitor_id]))\n",
    "    \n",
    "    print('predicted products: ')\n",
    "    display_item_ids(predicted_product_ids[0].numpy())\n",
    "    print('scores: ', score[0].numpy())\n",
    "\n",
    "\n",
    "user_id = get_random_customer_from_test_data()\n",
    "\n",
    "predict_user(user_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T07:54:14.932473Z"
    }
   },
   "id": "d8785689b14ad90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "user_id = next(iter(train.take(1)))['visitorid'].numpy()\n",
    "predict_user(user_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T07:54:14.933113Z"
    }
   },
   "id": "ccdee48c0a0d68ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save('../../models/retailrocket/rr_tensorflow_reco_3_epochs_v1.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T07:54:14.933851Z"
    }
   },
   "id": "eb395206e9a41cc7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_recommended_items"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-01T07:54:14.934536Z"
    }
   },
   "id": "313096ee48653126"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
